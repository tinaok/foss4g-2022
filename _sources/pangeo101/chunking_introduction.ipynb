{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bb79b-1b06-42c9-98d5-6185252c86e5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Authors & Contributors\n",
    "### Authors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)\n",
    "- Pier Lorenzo Marasco, Ispra (Italy), [@pl-marasco](https://github.com/pl-marasco)\n",
    "\n",
    "### Contributors\n",
    "- Anne Fouilloux, University of Oslo (Norway), [@annefou](https://github.com/annefou)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> Overview\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>Why do chunking matter?</li>\n",
    "        <li>How can I read datasets by chunks to optimize memory usage?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about chunking</li>\n",
    "        <li>Learn about zarr </li>\n",
    "        <li>Use kerchunk to consolidate chunk metadata and prepare single ensemble datasets for parallel computing</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "When dealing with large data files or collections, it's often impossible to load all the data you want to analyze into a single computer memory at once.  This is a situation where the Pangeo ecosystem can help you a lot. Xarray offers the possibility to work lazily on data __chunks__, which means pieces of an entire dataset.  By taking a data as 'chunk' we can process our data piece by piece on a single computer using local dask cluster, or distributed computing cluster.  How we will process these 'chunks' in a parallel enviroment will be discussed in [dask_introduction](./dask_introduction.ipynb)  The idia of __chunk__  will be explained here.\n",
    "\n",
    "When we process our data piece by piece, we would like to have our original data or processed result also save in 'chunks'. [Zarr](https://zarr.readthedocs.io/en/stable/) is a major way used in pangeo ecosystem to save our xarray dataset in __chunk__.\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/en/stable/)  is not the only file format which use __chunk__. We will also be using [kerchunk library](https://fsspec.github.io/kerchunk/) in this notebook to build a virtual __chunked__ dataset based on NetCDF, and show how it optimizes the access and analysis of large datasets.\n",
    "\n",
    "The analysis is very similar to what we have done in previous episodes, however we will use data on a global coverage and not only on a small geographical area (e.g. Lombardia).\n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using Global Long Term Statistics (1999-2019) products provided by the [Copernicus Global Land Service](https://land.copernicus.eu/global/index.html) and access them through [S3-comptabile storage](https://en.wikipedia.org/wiki/Amazon_S3) ([OpenStack Object Storage \"Swift\"](https://wiki.openstack.org/wiki/Swift)) with a data catalog we have created and made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c600794-dd9e-400b-bd09-dbb6e7039dad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following main Python packages:\n",
    "\n",
    "- fsspec {cite:ps}`d-fsspec-2018`\n",
    "- s3fs {cite:ps}`d-s3fs-2016`\n",
    "- xarray {cite:ps}`d-xarray-hoyer2017` with [`netCDF4`](https://pypi.org/project/h5netcdf/) and [`h5netcdf`](https://pypi.org/project/h5netcdf/) engines\n",
    "- dask {cite:ps}`d-dask-2016`\n",
    "- kerchunk {cite:ps}`d-kerchunk-2021`\n",
    "- geopandas {cite:ps}`d-geopandas-jordahl2020`\n",
    "- matplotlib {cite:ps}`d-matplotlib-Hunter2007`\n",
    "\n",
    "Please install these packages if not already available in your Python environment (see [Setup page](https://pangeo-data.github.io/foss4g-2022/before/setup.html)).\n",
    "\n",
    "### Packages\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4818f-2b6b-44c1-af77-97daf8a1c2a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global LTS\n",
    "\n",
    "In the previous episode, we used Long Term tastitics time-series for the region of Lombardy e.g. a very small area. Now we would like to use the original dataset that has a global coverage. Let us first open a single file (for January 1999-2019) to understand how much larger the global dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a418ed-c764-4577-803c-a6947db16966",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import s3fs\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bdead9-5f24-442e-93bf-715068fd330b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True,\n",
    "      client_kwargs={\n",
    "         'endpoint_url': 'https://object-store.cloud.muni.cz'\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9aeebc-db66-4469-8b3b-7b79fa24148d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As shown in the [Data discovery](./data_discovery.ipynb) chapter, when we have several files to read at once, we need to use Xarray `open_mfdataset` instead of `open_dataset`. It takes a list in input, and not a single element. We can also use `open_mfdataset` with one file as done below.\n",
    "\n",
    "Using `open_mfdataset` automatically switch from Numpy Arrays to Dask Arrays as the data structure used by Xarray. The same happens when specifying chunks in `open_dataset` as you'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a41ca7-ccd2-4223-ae15-d4119d7657cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3path = 's3://foss4g-data/CGLS_LTS_1999_2019/c_gls_NDVI-LTS_1999-2019-1221_GLOBE_VGT-PROBAV_V3.0.1.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af670e00-fbad-4f70-a24f-1b6dc7320a8f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS = xr.open_mfdataset([fs.open(s3path)])\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d2938-d0ea-41fe-b32c-af81a8055416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is __chunk__\n",
    "\n",
    "If you look carefully to `LTS`, each Data Variable is a `dask.array` with a chunsize of `(15680, 40320)`. So basically accessing one data variable would load arrays of dimensions `(15680, 40320)` into the memory.  You can have the information by clicking the icon indicated in the image below\n",
    "\n",
    "![Dask.array](../figures/datasize.png)\n",
    "\n",
    "When you open a netCDF file, by default, the chunks correspond to the entire size of the dataset originated to one file. When you need to analyze a considerable number of large files, the memory may not be sufficient anymore. This is where understanding chunking comes into play.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7dfb0c-5daa-41e8-bbb8-f453be29e1d7",
   "metadata": {},
   "source": [
    "__chunk__ is putting a dataset into small pieces. \n",
    "\n",
    "original data set is one piece, \n",
    "![Dask.array](../figures/notchunked.png)\n",
    "and we put them into pieces.  \n",
    "![Dask.array](../figures/chunked.png)\n",
    "\n",
    "We put them into pieces so that we can process our data block by block.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208b82b-363c-47b4-874c-f6a3a2bd06c1",
   "metadata": {},
   "source": [
    "Lets try to 'chunk' our data array LTS.nobs using following command.\n",
    "We would like to play with small size, so first we selct our data as you've learnd in [xarray_introduction](./xarray_introduction.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60ad82-9abb-42a6-9119-26f9b567b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=LTS.nobs.sel(lat=slice(80.,70.),lon=slice(70.,90))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8990f-5436-4723-a26b-bd396b19bb33",
   "metadata": {},
   "source": [
    "The test value have dimensions `(1121, 2240)`.  We will chunk it in a dimetion of 600x600 by following command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f644ba3-8eb5-4cf9-bd20-0aebda8bc05e",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test=test.chunk(600)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3ce5e-b604-4c3c-b081-f489ec4b68c9",
   "metadata": {},
   "source": [
    "As you can see in the above graphics we got 8 __chunk__.  These 8 chunks are noted as (0,0) ..(0,3) (1,0),..(1,3) as you can see in the next visualisation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad28ce2-e75e-46a6-83f0-e953ced92dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231226cf-027b-4f6a-8558-415e182336bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see in the [graphic](../figures/chunkingprocess.png)\n",
    "When selecting you may have the feeling that the chunk sizes changes. In fact Xarray will still have to fetch the entire initial dataset (LTZ) to perform the selection on any of the Data variables. Again it will not be very optimal (your Python Jupyter kernel may crash too!) with large numbers of files and large files. \n",
    "This brings to our next subject [Zarr](https://zarr.readthedocs.io/en/stable/) \n",
    "If we can have our oribinal data already 'chunked' we do not need to load entire dataset.  \n",
    "Lets try to save our test data in a zarr format to learn what zarr is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21442c0-68ba-41be-ba5f-8ddbd8ba174f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.to_dataset().to_zarr('test.zarr',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befcc32-db12-4c8f-8a8d-20f97e8b6712",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Attention</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>DataArray can not be saved as 'zarr'.  Before saving your data to zarr, you will need to convert that to a DataSet</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8993f7a-80ef-4962-b18b-f8071950add8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Attention</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>You can try to explore the zarr file you just created using `ls -la test.zarr` and  `ls -la test.zarr/nobs `</li>\n",
    "        <li>Did you find the __chunks__ you just created in your zarr file? </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ebee54-bbd2-416b-b5b3-7965542cdaa2",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!ls -al test.zarr/nobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4e44f-66fa-4a26-b5c3-69197b90cc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01df0462-9ad3-4e2e-855f-fcdb06b456a7",
   "metadata": {},
   "source": [
    "## opening multiple file and Kerchunk\n",
    "\n",
    "As shown in the [Data discovery](./data_discovery.ipynb) chapter, when we have several files to read at once, we need to use Xarray `open_mfdataset`. When using open_mfdataset for example to NetCDF files, each NetCDF file is automatically considerd as 'one chunk'.  Open_mfdataset analyse each NetCDF files performs multiple operations, like concartenate the coordinate, checking compatibility,..  which is sometime time consuming, and sometime we use exactry same set of input files for different analysis.  [kerchunk library](https://fsspec.github.io/kerchunk/) can build virtual zarr libraries and can load multiple files as a set of 'zarr' file.\n",
    "\n",
    "But that is not the only optimisation  kerchunk brings to pangeo ecosystem.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29025fb7-3fc0-499e-9117-9377a0899f67",
   "metadata": {},
   "source": [
    "## Exploiting native chunking for reading datasets\n",
    "\n",
    "Many data formats (for instance,  [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) or [netCDF4](https://unidata.github.io/netcdf4-python/) with HDF5 backend,[geoTIFF](https://en.wikipedia.org/wiki/GeoTIFF)) have chunk cabability.  It is done at the creation of each files.  Lets call that as 'natvie __chunks__'.  These native chunks can be retrieved and used when opening and accessing the files. This will allow significantly reducing the amount of memory when analyzing Data Variables (only the needed chunks will be transferred, and if all the data have to be accessed it can be serializable e.g. chunks are processed one after the other).\n",
    "\n",
    " [kerchunk library](https://fsspec.github.io/kerchunk/) can extract these chunk information, and kerchunk can combine group of these information and can create virtual zarr libraries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae359620-527f-4525-9cba-3b8cc6754750",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extract chunk information\n",
    "\n",
    "We extract native chunk information from each NetCDF file using `kerchunk.hdf`.\n",
    "Let's start with a single file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c06a382f-d924-40bd-a3bb-a82a7dcc0484",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import kerchunk.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6767b56-d17b-48f4-9718-ee87207e98c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use `kerchunk.hdf` because our files are written in `netCDF4`  format which is based on HDF5 and `SingleHdf5ToZarr` to translate the metadata of one HDF5 file into Zarr metadata format. The parameter `inline_threshold` is an *optimization* and tells `SingleHdf5ToZarr` to include chunks smaller than this value directly in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdce787-38f6-4dc1-99a7-31e6e3925d61",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "remote_filename = 'https://object-store.cloud.muni.cz/swift/v1/foss4g-data/CGLS_LTS_1999_2019/c_gls_NDVI-LTS_1999-2019-1221_GLOBE_VGT-PROBAV_V3.0.1.nc'\n",
    "with fsspec.open(remote_filename) as inf:\n",
    "    h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, remote_filename, inline_threshold=100)\n",
    "    chunk_info = h5chunks.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7d185-8efa-40cb-8625-a5589b32bed4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's have a look at `chunk_info`. It is a Python dictionary so we can use `pprint` to print it nicely.\n",
    "\n",
    "Content is a bit complicated, but it's only metadata in Zarr format indicating what's in the original file, and where the chunks of the file are located (bytes offset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80eacc19-c075-492b-8efc-fefe735b591b",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(chunk_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f73e1-05a6-46eb-a867-6b0f49a81df1",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After we have collected information on the native chunks in the original data file and consolidated our Zarr metadata, we can open the files using `zarr` and pass this chunk information into a storage option. We also need to pass `\"consolidated\": False` because the original dataset does not contain any `zarr` consolidating metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c82da0-e286-463d-a801-04a651102029",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\": chunk_info,\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae4b55-df4f-4829-9142-e06c2b910c1f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can notice above, all the Data Variables have several chunks. The native chunking of the NetCDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f407f0-f9b8-4320-bed1-5b77c14949fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Combine all LTS files into one kerchunked single ensemble dataset\n",
    "\n",
    "Now we will combine all the files into one kerchunked consolidated dataset, and try to open it as a xarray dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceabe66-09bc-42a8-9469-edabd88791bf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us first collect the chunk information for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8bd3546-5431-4d64-9f23-24cfe90489a6",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fs.ls('foss4g-data/CGLS_LTS_1999_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccfe5c6a-4471-4481-8a61-4e17bdd4d414",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e76758-25fb-49f4-86c5-f8f18275fb81",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3path = 's3://foss4g-data/CGLS_LTS_1999_2019/c_gls_*.nc'\n",
    "chunk_info_list = []\n",
    "time_list = []\n",
    "\n",
    "for file in fs.glob(s3path):\n",
    "    url = 'https://object-store.cloud.muni.cz/swift/v1/' + file\n",
    "    t = datetime.strptime(file.split('/')[-1].split('_')[3].replace('1999-', ''), \"%Y-%m%d\")\n",
    "    time_list.append(t)\n",
    "    with fsspec.open(url) as inf:\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, url, inline_threshold=100)\n",
    "        chunk_info_list.append(h5chunks.translate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf9ea0-c8c8-4fe9-a753-7558f587fb76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This time we use `MultiZarrToZarr` to combine multiple kerchunked datasets into a single logical aggregated dataset. Like when opening multiple files with Xarray `open_mfdataset`, we need to tell `MultiZarrToZarr` how to concatenate all the files. There is no time dimension in the original dataset, but one file corresponds to one date (average over the period 1999-2019 for a given 10-day period e.g. January 01, January 11, January 21, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a232d-01c6-4590-af85-63790df41024",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kerchunk.combine import MultiZarrToZarr\n",
    "mzz = MultiZarrToZarr(\n",
    "    chunk_info_list,\n",
    "    coo_map={'INDEX': 'INDEX'},\n",
    "    identical_dims=['crs'],\n",
    "    concat_dims=[\"INDEX\"],\n",
    ")\n",
    "\n",
    "out = mzz.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d85033-1090-47f7-a302-73ddfdf346fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we can open the complete dataset using our consolidated Zarr metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ef1e5-4fde-4f73-8203-9481528603c8",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\": out,\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844f7b7-4e7a-4d52-a4bb-076bb71226fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can save the consolidated metadata for our dataset in a file, and reuse it later to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec340712-b9b6-48a4-be9a-37dd30b8f188",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8f485-230f-491a-a964-0b7357507734",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jsonfile='c_gls_NDVI-LTS_1999-2019.json'\n",
    "with open(jsonfile, mode='w') as f :\n",
    "    json.dump(out, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a80c9-2cfb-4ccf-8df1-4704874689a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can then load data from this catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0faea-66fb-4210-b119-bbbeeec47560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\":'./c_gls_NDVI-LTS_1999-2019.json',\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880c571-92c5-4b25-8a1c-0bad61b6175e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also postprocess LTS to get the dates properly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f1e82-8041-46ff-95f1-f3e82d81cef1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS = LTS.rename({'INDEX': 'time'}).assign_coords(time=time_list)\n",
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffd88b-6eb8-45d9-ad26-fdb3ca7af507",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d1165-7b57-45fd-b9ed-05046ce286c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    GAUL = gpd.read_file('Italy.geojson')\n",
    "except:\n",
    "    GAUL = gpd.read_file('zip+https://mars.jrc.ec.europa.eu/asap/files/gaul1_asap.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363bafb-551a-4a25-8793-b854cefc071f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "AOI_name = 'Lombardia'\n",
    "AOI = GAUL[GAUL.name1 == AOI_name]\n",
    "AOI_poly = AOI.geometry\n",
    "AOI_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe014a-8bd8-4b9e-8a4b-c3cdda63833b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS = LTS.sel(lat=slice(46.5,44.5), lon=slice(8.5,11.5))\n",
    "LTS = LTS.rio.write_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbfab2-a11a-46f2-858c-4c336ad76bc7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS = LTS.rio.clip(AOI_poly, crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503edd2-cfc7-4f8c-a06e-481e63527ba9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35969623-39a1-4ea2-bf4b-136b2225ce61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b29622-c979-4e42-873a-366d4660e313",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize LTS statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb92583-615c-4e5b-b773-21da14c932fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad179317-353b-4bcd-9b1d-604572a2129e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[15,5])\n",
    "\n",
    "# Fix extent\n",
    "minval = 0.0\n",
    "maxval = 0.9\n",
    "\n",
    "itime=0 # plot the first date\n",
    "\n",
    "# Plot 1 for min subplot argument (nrows, ncols, nplot)\n",
    "# here 1 row, 2 columns and 1st plot\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "LTS.isel(time=itime)['min'].plot(ax=ax1)\n",
    "# Plot 2 for max\n",
    "# 2nd plot \n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "LTS.isel(time=itime)['max'].plot(ax=ax2)\n",
    "\n",
    "# Title for both plots\n",
    "fig.suptitle('LTS NDVI statistics (Minimum and Maximum)', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509a57d-2960-48f9-a9bb-2e163477c83e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The catalog (json file we created) can be shared on the cloud (or github, etc.) and anyone can load it from there too.\n",
    "This approach allows anyone to easily access LTS data and select the Area of Interest for their own study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665722c-4c65-4334-987e-1f5bc4841036",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Understanding chunking is key to optimize your data analysis. In this episode we learned how to optimize the memory by exploiting native chunks from netCDF4 data files and instructing Xarray to access data per chunk. However, computations can be very slow and to optimize the computational part we need to parallelize our data analysis. This is what you will learn in the next episode with Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1f23a-8838-41fe-96d7-4f7b0fb9cc3f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Key Points</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Chunking and compression</li>\n",
    "        <li>kerchunk</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"chunking\" and topic % \"package\"\n",
    ":keyprefix: d-\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
